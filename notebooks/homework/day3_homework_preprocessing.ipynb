{"cells":[{"cell_type":"markdown","metadata":{"id":"c2XoLzfpiiz4"},"source":["### Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-oiDfRgiiz6"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mon3mdAPiiz8"},"outputs":[],"source":["# get dataset\n","loans_df = pd.read_csv('../../data/loans_day2.csv', index_col=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TG7VutsWiiz8","outputId":"be039075-1dd1-4d09-edcf-82724f9e0963"},"outputs":[],"source":["#show first five lines\n","loans_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Note! If you have `purpose` as a column here, please delete it to make this notebook run completely."]},{"cell_type":"markdown","metadata":{"id":"kCJ8wAr3iiz-"},"source":["### Preprocessing\n","\n","Before we dive into the modelling we'll need to talk a bit about preprocessing first!\n","\n","You might not expect it but data-preprocessing is one the most import aspects of Data Science and therefore also one of the most time-consuming."]},{"cell_type":"markdown","metadata":{"id":"B0OkaY_cjWAk"},"source":["![img](https://docs.google.com/uc?export=download&id=1JQuyBRxSWh90xIuxGU12cIAVyMOoI4aO)"]},{"cell_type":"markdown","metadata":{"id":"i2TJp6Zsjb2O"},"source":["Let's look at this chart which visualises the spreak of workload for a common data scientist.\n","\n","We can clearly see the actual modelling only takes up a small amount of time on a daily basis and that the steps before take up a lot more time.\n","\n","We already talked about data-sourcing or collecting data-sets in the last lecture. So let's focus on the data cleaning and preprocessing now.\n","\n","**Data cleaning** is relatively straightforward. Here we make sure our dataset does not include `missing values` and/or `duplicates`. Duplicates we will off course remove. We can do the same for missing values but there's [other ways](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) to handle these (which we will not discuss here).\n","\n","Another part of our cleaning is to potentially remove noisy data such as `outliers`. This will again not be discussed here but you can find more information [here](https://medium.com/analytics-vidhya/how-to-remove-outliers-for-machine-learning-24620c4657e8)\n","\n","After we've cleaned our data we move on to **data preprocessing**. Here we will transform the data to a context where a machine can work with them. An example you will see a bit later is `scaling`where we use statistical methods to put all our variables on the same scale and hence generally improving the performance of our models. Another will be handling categorical or text-data."]},{"cell_type":"markdown","metadata":{"id":"iVnBMdo-iiz_"},"source":["#### Data cleaning"]},{"cell_type":"markdown","metadata":{"id":"NrQ_Sq69ii0A"},"source":["##### 1. Duplicates"]},{"cell_type":"markdown","metadata":{"id":"J-bKvDCMii0A"},"source":["In any preprocessing workflow you would tackle `duplicates` first. You can check `.duplicated().sum()`to see how many duplicate rows you have. Next you can remove them with `.drop_duplicates()`. Let's check the cell below. If it's 0 we've taken care of it for you.. if not good luck!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UtFUnQ80ii0A","outputId":"256cc538-d9a9-4cde-df36-217807a94603"},"outputs":[],"source":["loans_df.duplicated().sum()"]},{"cell_type":"markdown","metadata":{"id":"-Ui6qxC7ii0B"},"source":["##### 2. Null values/Missing data"]},{"cell_type":"markdown","metadata":{"id":"kR0IW7rMii0B"},"source":["After the duplicats you would normally handle missing data or `null-values`. Again we've already taken care of this for you. Check out below coding cell to see the % of `null-values` per column as proof!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99oZj3WIii0B","outputId":"123ac2ce-2c9e-4f7d-8adb-4c23de9f40c6"},"outputs":[],"source":["loans_df.isnull().sum()/len(loans_df)"]},{"cell_type":"markdown","metadata":{"id":"LfHzhMG7ii0B"},"source":["#### Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"7UpVlx98ii0C"},"source":["#### 1. Encoding"]},{"cell_type":"markdown","metadata":{"id":"MPZUitLrii0C"},"source":["You might have already noticed that we have some columns containing text. ML models off course can't handle text but no worries! We'll handle these quickly by using encoding."]},{"cell_type":"markdown","metadata":{"id":"KTMT-Xc0kXLG"},"source":["#### Ordinal encoding"]},{"cell_type":"markdown","metadata":{"id":"b677MD47khmd"},"source":["![img](https://docs.google.com/uc?export=download&id=12BTL1wqY9qvolONkwDQCL7AqzFZcsi2H)"]},{"cell_type":"markdown","metadata":{"id":"jBjQIkMEii0C"},"source":["These are `ordinal variables` where we can assign an assign an inherent importance to each variable. Hence for these variables **order matters**.\n","This means that it makes sense for us in this context to assign higher numbers to categories of higher importance. That way our ML algorithm can pick up on this hierachy.\n","\n","Examples of this in real life would be level of education, customer satisfaction or in our example `grade`."]},{"cell_type":"markdown","metadata":{"id":"NhBDWcbpii0C"},"source":["`grade` has 7 unique values, all representing a specific grade that is related to the client's solvency."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztKHM38Lii0C","outputId":"dfe34c9c-b396-4e8c-929b-9363ba4bcdfd"},"outputs":[],"source":["loans_df.grade.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBP64UCKii0D","outputId":"51e48744-d2ed-4924-822a-633670f80c08"},"outputs":[],"source":["loans_df.grade.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"jUguWDYAii0D"},"source":["Makes sense that there is a structure here and that someone with grade A would have more change of paying back their loan compared to someone with an F grade.\n","\n","Let's use the sklearns `OrdinalEncoder` class to add this hierarchy in our dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fyZiv6exii0D","outputId":"5fa5c60e-679a-498f-fc89-9fad542a95c7"},"outputs":[],"source":["from sklearn.preprocessing import OrdinalEncoder\n","\n","#specify list of lists in order\n","cat = [['G', 'F', 'E', 'D', 'C', 'B', 'A']]\n","\n","#instantiate encoder\n","ord_enc = OrdinalEncoder(categories=cat)\n","\n","#fit encoder on grade\n","ord_enc.fit(loans_df[['grade']])\n","\n","#transform grade variable\n","loans_df['grade'] = ord_enc.transform(loans_df[['grade']])\n","\n","loans_df.grade.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"tevfOD-pk7D2"},"source":["#### Feature encoding"]},{"cell_type":"markdown","metadata":{"id":"qMKTfBSak96I"},"source":["\n","![img](https://docs.google.com/uc?export=download&id=1Z_kta3r_2IeUeF80W_qkIr8maFbaJa6N)"]},{"cell_type":"markdown","metadata":{"id":"SH8_vtxHii0D"},"source":["These are `nominal variables` where there's not really an inherent hierarchy as all possibilites are of equal importance. Hence for these variables **order does not matter**\n","Here we have to apply a different technique as above because we want our ML algorithm to pick up on the fact that all these variables are of equal importance. This technique, that will create a binary column for each individual option, is referred to as `on-hot-encoding`.\n","Examples of this in real life would be state, gender or in our example (althoug slightly debatable) `home ownership`."]},{"cell_type":"markdown","metadata":{"id":"oGIg97-rii0D"},"source":["`home_ownership` has 6 unique values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g3xiya6pii0D","outputId":"da7035ba-b162-4f49-ecbe-b68103bf9e97"},"outputs":[],"source":["loans_df.home_ownership.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"SPU8n9-iii0E"},"source":["Of our 6 values we're only really interested in the top 3. Use `.isin()`to filter the DF on multiple conditions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9vOkDH4hii0E"},"outputs":[],"source":["loans_df = loans_df[loans_df['home_ownership'].isin(['MORTGAGE','RENT', 'OWN'])]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WpV1ObAYii0E","outputId":"5757abec-5caa-4989-e623-2fa8759c6f0a"},"outputs":[],"source":["loans_df.home_ownership.unique()"]},{"cell_type":"markdown","metadata":{"id":"p2KS7811ii0E"},"source":["Let's use the `one_hot_encoder` to transfer these values to all seperate binary columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xesSjiNXii0E","outputId":"7945ad49-ec3f-4332-bbe9-69dc7b5d3b1f"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder\n","import numpy as np\n","\n","# Instantiate encoder\n","ohe = OneHotEncoder(sparse = False)\n","\n","# Fit encoder\n","ohe.fit(loans_df[['home_ownership']]) \n","\n","# Encode ownership\n","home_encoded = ohe.transform(loans_df[['home_ownership']]) \n","\n","# Transpose encoded ownership back into dataframe -> be sure to check .unique() to figure out the order of the columns\n","loans_df[\"mortgage\"],loans_df[\"own\"],loans_df['rent'] = home_encoded.T \n","\n","#drop original columns\n","loans_df.drop(columns='home_ownership', inplace=True)\n","\n","loans_df.head()"]},{"cell_type":"markdown","metadata":{"id":"qWzGulN9ii0E"},"source":["Now that we've gotten used to the `one_hot_encoder` let's take a look at the column **term** as well. Here we have two options so we can transform it to one binary columns with one value being 1 and the other being 0.\n","\n","We can use the parameter `drop = 'if_binary'` to get our desired result."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oi8j_hLLii0E","outputId":"cd8a19f7-c86e-478f-b751-f32dc35b3d6e"},"outputs":[],"source":["loans_df.term.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eslWn20Uii0E","outputId":"3ea25834-c135-4b2b-812c-90dd14ec4783"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder\n","import numpy as np\n","\n","# Instantiate encoder with extra param\n","ohe = OneHotEncoder(sparse = False, drop='if_binary')\n","\n","# Fit encoder\n","ohe.fit(loans_df[['term']]) \n","\n","# Encode alley\n","home_encoded = ohe.transform(loans_df[['term']]) \n","\n","# Transpose encoded term back into dataframe by overwriting the original column\n","loans_df[\"term\"] = home_encoded\n","\n","\n","loans_df.head()"]},{"cell_type":"markdown","metadata":{"id":"r6wVjdOUii0F"},"source":["**loan_status** will be our target for our classification task. We can transform this one using the `one_hot_encoder` as before or we can also use the `label_encoder` which is specifically designed for this purpose."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XvhVCxbIii0F","outputId":"33dd0743-f40b-4431-fc76-08cf9077a5ca"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","label = LabelEncoder()\n","\n","label.fit(loans_df['loan_status'])\n","\n","loans_df['loan_status'] = label.transform(loans_df['loan_status'])\n","\n","loans_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snPSspgyii0F","outputId":"78060d46-4faa-4869-fb6c-2e439d417a59"},"outputs":[],"source":["loans_df['loan_status'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"MdKKPrDgii0F"},"source":["Let's save our preprocessed dataset before we move on to scaling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCVZm83Fii0F"},"outputs":[],"source":["loans_df.to_csv('../../data/loans_day3.csv')"]},{"cell_type":"markdown","metadata":{"id":"VJ6RsBDfii0F"},"source":["#### 2. Feature scaling"]},{"cell_type":"markdown","metadata":{"id":"vFj5fgPXlgMA"},"source":["![img](https://docs.google.com/uc?export=download&id=1AfP4BzVxlQ2Kr16YZgPwUuHCCJBOSSU0)"]},{"cell_type":"markdown","metadata":{"id":"W9Ssvu32ii0F"},"source":["Scaling is also very important in our preprocessing. Putting variables on the same scale will allow our model to treat every column equally and not give more importance to relatively larger values (like `annual_inc` vs `emp_length` for example).\n","Always scale when doing any type or application of ML. It'll greatly improve your model!\n","There are quite some scalers out there but let's use the **standard_scaler** to keep it simple. Here we use the mean and standard deviation of every feature to scale these features so that it has a mean value of 0 and a standard deviation of 1."]},{"cell_type":"markdown","metadata":{"id":"2aO6Qgzkii0F"},"source":["Let's check first the distribution of `int_rate` and `installment`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tEFcmzxfii0F","outputId":"b9a138e6-29d2-42bb-ce2d-a9fdb187b05e"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.hist(loans_df['int_rate']);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kgw8Xj3dii0F","outputId":"5fe9c739-702e-4dc4-ec20-5d43ff275eff"},"outputs":[],"source":["plt.hist(loans_df['installment']);"]},{"cell_type":"markdown","metadata":{"id":"3HdmHgnWii0G"},"source":["You can clearly see that these variables have a similar distribution but are on a different scale. \n","\n","Scaling these variables will have multiple effects for our model:\n","- it will make sure that `installment` does not outweigh `int_rate` just purely based on it's scale\n","- the smaller values will make our model computationally more efficient\n","- it will increase the interpretability of feature coefficients"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GX5al7-kii0G"},"outputs":[],"source":["#define x & y because we don't wanna scale our target\n","from sklearn.model_selection import train_test_split\n","\n","X = loans_df.drop(columns='loan_amnt')\n","\n","y = loans_df.loan_amnt\n","\n","#train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BqYpCc7vii0G","outputId":"5e2fa3c2-3730-4daf-b9fd-75ae48e5d541"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","\n","# Instanciate StandarScaler\n","scaler = StandardScaler() \n","\n","# Fit scaler to data\n","scaler.fit(X_train)\n","\n","# Use scaler to transform data\n","X_train_scaled = scaler.transform(X_train) \n","\n","# create df to show output\n","X_train_scaled_df = pd.DataFrame(X_train_scaled, columns = scaler.get_feature_names_out())\n","\n","X_train_scaled_df.head()"]},{"cell_type":"markdown","metadata":{"id":"ABLwPmUeii0G"},"source":["Now let's check `int_rate` and `installment` again. You see that while the distribution remained the same, the scale changed right?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TKirQlM2ii0G","outputId":"58059662-86f3-46ab-8bc1-401980f8cb5d"},"outputs":[],"source":["plt.hist(X_train_scaled_df['int_rate']);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUw5sr1eii0G","outputId":"6e3337f5-8a1c-4378-8b6b-cb46ed87012b"},"outputs":[],"source":["plt.hist(X_train_scaled_df['installment']);"]},{"cell_type":"markdown","metadata":{"id":"kZsxMdHGii0G"},"source":["### Check performance"]},{"cell_type":"markdown","metadata":{"id":"Z-KqOQllii0G"},"source":["Now let's check the difference in performance. For this we'll use a new model!\n","\n","K-Nearest Neighbors (KNN) is a non-linear, distance based model capable of solving both regression and classification tasks.\n","\n","*   Looks at K closest samples to make a prediction\n","*   Up to us to determine K (hyperparameter)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VOlzKcdzqW2N"},"source":["![img](https://docs.google.com/uc?export=download&id=1zPhJFC1Y4rUcHIRbhyAWRs3I8924Pp-_)"]},{"cell_type":"markdown","metadata":{"id":"dcR4xo6rqoO9"},"source":["![img](https://docs.google.com/uc?export=download&id=1tJgdoNwaZ8IzhyUNUQtyUSry5L3SvUFr)"]},{"cell_type":"markdown","metadata":{"id":"bTQaDu6OqoXI"},"source":["![img](https://docs.google.com/uc?export=download&id=1XdHcWnahZQKEaa9w7SUGVvYPG3wjUGkd)"]},{"cell_type":"markdown","metadata":{"id":"TfQaGOrxqve4"},"source":["![img](https://docs.google.com/uc?export=download&id=1MOWK38ksmco27lv95UEEV8Ynk23WUIKC)"]},{"cell_type":"markdown","metadata":{"id":"QkhIE1Q1qyzG"},"source":["![img](https://docs.google.com/uc?export=download&id=1-fkfQ4OHTX2fjfdWxZz_Z7i6V9fjp13b)"]},{"cell_type":"markdown","metadata":{"id":"7PH3sWpJq9d3"},"source":["![img](https://docs.google.com/uc?export=download&id=1wEKxO0BZ_-ZQpb2V7qfUsFCg_AiFCUyd)"]},{"cell_type":"markdown","metadata":{"id":"OyP2uF89rdGW"},"source":["Let's first run a regression with our unscaled variables:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWx55XG4ii0G","outputId":"13f70d0c-21b2-4656-e959-52b8c59518ed"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","\n","knn = KNeighborsRegressor()\n","\n","knn.fit(X_train, y_train)\n","\n","knn.score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"p1zS3yr6rh0P"},"source":["Now let's run one with our scaled variables to see the difference in performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"93dcKbLZii0G","outputId":"97c9888b-60d2-48d7-c526-49bdb89b9c4a"},"outputs":[],"source":["knn.fit(X_train_scaled, y_train)\n","\n","X_test_scaled = scaler.transform(X_test)\n","\n","knn.score(X_test_scaled, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["The score was already quite high for our initial model but you do see a clear increase by doing the scaling. Since most models are based on some kind of distance.. scaling will help to improve the performance of those models."]}],"metadata":{"colab":{"collapsed_sections":[],"name":"day3_preprocessing_bonus.ipynb","provenance":[]},"interpreter":{"hash":"63305f572d39f1f057a29d6f90b97f50226f939806bf67798e7b1d2d4622b84a"},"kernelspec":{"display_name":"Python 3.8.12 64-bit ('lewagon')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
