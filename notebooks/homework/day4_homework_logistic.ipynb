{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXc07XfbxCAd"
      },
      "source": [
        "# Fraud Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-G-zdvsLsYw"
      },
      "source": [
        "In this exercise, we work on tabular data of credit card history available on [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud). The aim is to detect a mere 492 fraudulent transactions from 284,807 transactions in total.\n",
        "\n",
        "One particularity of this dataset is it's proportion of positive labels which is quite low, we say it's a highly imbalanced dataset since the number of examples in one class greatly outnumbers the examples. We'll see how to deal with such a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "NcZ0pUcPLH7V",
        "outputId": "c491069b-1de9-4435-fec3-ac205541569a"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "Eidr7kJID2C6",
        "outputId": "5373c2c7-e036-4b21-d571-c02bc9391da5"
      },
      "outputs": [],
      "source": [
        "# This is to fix bugs seaborn has with new matplotlib version\n",
        "!pip install matplotlib==3.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AuhBJ4CLp88"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69W-FK8UyKfc"
      },
      "outputs": [],
      "source": [
        "# This will fix the figsize for the whole notebook\n",
        "mpl.rcParams['figure.figsize'] = (12, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f5NaFcRMLZq"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YKUBF6WMSu5"
      },
      "source": [
        "### Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "wiSre6FtMUf2",
        "outputId": "1d9014de-9b1e-4a1e-fd43-6bf2b46baf7f"
      },
      "outputs": [],
      "source": [
        "file = tf.keras.utils\n",
        "raw_df = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')\n",
        "raw_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlUbcqeNMYZb"
      },
      "source": [
        "### Inspecting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F5AFPJWMctv"
      },
      "outputs": [],
      "source": [
        "# TODO: inspect the shape, format, basic statistics and proportion of positive sample of the data\n",
        "# (both for input and output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBkCnAtmxUPi"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "print(raw_df.shape)\n",
        "fraud_fraction = 100 * raw_df.Class.sum() / len(raw_df)\n",
        "print(f'{fraud_fraction} % of fraud in the dataset')\n",
        "raw.describe()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P-VfYfhNtyL"
      },
      "source": [
        "### Cleaning the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg4-L6WDN0IS"
      },
      "outputs": [],
      "source": [
        "# TODO: Inspect and clean column by column:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7rCpgzixg4Q"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "# hints:\n",
        "# - do you need the time column?\n",
        "# - what do you think about the range of the column Amount?\n",
        "\n",
        "cleaned_df = raw_df.copy()\n",
        "\n",
        "# You don't want the `Time` column.\n",
        "cleaned_df.pop('Time')\n",
        "\n",
        "# The `Amount` column covers a huge range. Convert to log-space.\n",
        "eps=0.001 # 0 => 0.1Â¢\n",
        "cleaned_df['Amount'] = np.log(cleaned_df['Amount'] + eps)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwX-E-z4OTx7"
      },
      "outputs": [],
      "source": [
        "# Here, we split the data in (train, val, test) with (0.64, 0.16, 0.20),\n",
        "# i.e. test is 20% of all, train is 80% of all and val is 80% of train\n",
        "train_df, test_df = train_test_split(cleaned_df, test_size=0.2)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
        "\n",
        "train_labels = train_df.pop('Class').values\n",
        "val_labels = val_df.pop('Class').values\n",
        "test_labels = test_df.pop('Class').values\n",
        "\n",
        "train_features = train_df.values\n",
        "val_features = val_df.values\n",
        "test_features = test_df.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDyYn7osPGFs"
      },
      "outputs": [],
      "source": [
        "# TODO: Fit a standard scaler to your training data and apply it to the test, val data\n",
        "# to transform you train data in 0-mean and 1-std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzKzks58xkyN"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "Use the StandardScaler from sklearn to be more efficient and robust to outliers. Fit on the train data and apply train and test data.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "# hints:\n",
        "scaler = StandardScaler()\n",
        "train_features = scaler.fit_transform(train_features)\n",
        "\n",
        "val_features = scaler.transform(val_features)\n",
        "test_features = scaler.transform(test_features)\n",
        "\n",
        "train_features = np.clip(train_features, -5, 5)\n",
        "val_features = np.clip(val_features, -5, 5)\n",
        "test_features = np.clip(test_features, -5, 5)\n",
        "\n",
        "\n",
        "print('Training labels shape:', train_labels.shape)\n",
        "print('Validation labels shape:', val_labels.shape)\n",
        "print('Test labels shape:', test_labels.shape)\n",
        "\n",
        "print('Training features shape:', train_features.shape)\n",
        "print('Validation features shape:', val_features.shape)\n",
        "print('Test features shape:', test_features.shape)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5uvo9iDPbqv"
      },
      "source": [
        "### Checking features distributions\n",
        "\n",
        "Next compare the distributions of the positive and negative examples over a few features. Good questions to ask yourself at this point are:\n",
        "\n",
        "Do these distributions make sense?\n",
        "\n",
        "==> Yes. You've normalized the input and these are mostly concentrated in the +/- 2 range.\n",
        "\n",
        "Can you see the difference between the ditributions?\n",
        "\n",
        "==> Yes the positive examples contain a much higher rate of extreme values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "iow8zlxIwZEn",
        "outputId": "d92d1fd7-8d94-4808-e924-6a8a2a5484e2"
      },
      "outputs": [],
      "source": [
        "bool_train_labels = (train_labels != 0)\n",
        "\n",
        "# We recreate a dataframe to plot the processed features and not the raw one\n",
        "pos_df = pd.DataFrame(train_features[ bool_train_labels], columns = train_df.columns)\n",
        "neg_df = pd.DataFrame(train_features[~bool_train_labels], columns = train_df.columns)\n",
        "\n",
        "sns.jointplot(pos_df['V5'], pos_df['V6'],\n",
        "              kind='hex', xlim = (-5,5), ylim = (-5,5))\n",
        "\n",
        "plt.suptitle(\"Positive distribution\")\n",
        "\n",
        "sns.jointplot(neg_df['V5'], neg_df['V6'],\n",
        "              kind='hex', xlim = (-5,5), ylim = (-5,5))\n",
        "_ = plt.suptitle(\"Negative distribution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6AkytvdyA_K"
      },
      "source": [
        "#Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYVpSUhD0LKY"
      },
      "source": [
        "### Defining Model & Metrics\n",
        "\n",
        "Notes about metrics:\n",
        "\n",
        "- False negatives and false positives are samples that were incorrectly classified\n",
        "\n",
        "- True negatives and true positives are samples that were correctly classified\n",
        "\n",
        "- Accuracy is the percentage of examples correctly classified:\n",
        "$\\frac{true \\, positive}{n_{samples}}$\n",
        "\n",
        "- Precision is the percentage of predicted positives that were correctly classified\n",
        "$\\frac{true \\, positive}{true \\, positive + false \\, positive}$\n",
        "\n",
        "- Recall is the percentage of actual positives that were correctly classified\n",
        "$\\frac{true \\, positive}{true \\, positive + false \\, negative}$\n",
        "\n",
        "- AUC refers to the Area Under the Curve of a Receiver Operating Characteristic curve (ROC-AUC). This metric is equal to the probability that a classifier will rank a random positive sample higher than than a random negative sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1yYa5FC0N_R"
      },
      "outputs": [],
      "source": [
        "# TODO: build a function build_model() to build a relevant model for this task\n",
        "# Here we ask you to pass to your model compiler a list of relevant metrics\n",
        "# to evaluate classification task, don't hesitate to include many of them,\n",
        "# it will help you to evaluate your model on this imbalanced dataset\n",
        "# check keras.metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OiaihX6xsF_"
      },
      "source": [
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "\n",
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "\n",
        "def build_model(metrics = METRICS):\n",
        "  model = keras.Sequential([\n",
        "      keras.layers.Dense(\n",
        "          16, activation='relu',\n",
        "          input_shape=(train_features.shape[-1],)),\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.Dense(1, activation='sigmoid'),\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(lr=1e-3),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djirZzVJ0bn6"
      },
      "outputs": [],
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AVgSWeM2T4f"
      },
      "outputs": [],
      "source": [
        "# TODO: a good practice, after printing the summary of your model, is to\n",
        "# predict with your model before training it to make\n",
        "# sure it runs through and the output is correctly formatted\n",
        "# Predict on 10 input sample to check that"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO3hwoeXxtCZ"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "model.predict(train_features[:10])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgTG6ga72snc"
      },
      "source": [
        "What do you think about the intial prediction done by the models? \n",
        "\n",
        "Are they similar to what you expect?\n",
        "\n",
        "Since we have a highly imbalanced dataset, we would rather see intial predictions very low, so we can change this before training our model so it's already clother to the truth and will converge faster\n",
        "\n",
        "see here for a reference of this trick, part \"init well\": [A Recipe for Training Neural Networks: \"init well\"](http://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h805ixO73QkP"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement the suggested trick, you just need to set the initial bias\n",
        "# and check on initial predictions it does what you expect\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEIdwdkDxugi"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "To set the bias you can use tf.keras.initializers.Constant\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "pos = bool_train_labels.sum()\n",
        "neg = (~bool_train_labels).sum()\n",
        "\n",
        "initial_bias = np.log([pos/neg])\n",
        "\n",
        "def build_model(metrics = METRICS, output_bias=None):\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "  model = keras.Sequential([\n",
        "      keras.layers.Dense(\n",
        "          16, activation='relu',\n",
        "          input_shape=(train_features.shape[-1],)),\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.Dense(1, activation='sigmoid',\n",
        "                         bias_initializer=output_bias),\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(lr=1e-3),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "print(initial_bias)\n",
        "model = build_model(output_bias = initial_bias)\n",
        "model.predict(train_features[:10])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRTiZt8i3QoA"
      },
      "source": [
        "What do you observe? Is it closer to the expected value of $\\frac{pos}{total} = 0.0018$ ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHiATp3b1CPR"
      },
      "source": [
        "### Training: Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DNdxWPTw4wLo",
        "outputId": "3bbd0d2f-050e-42be-c71e-a37f27b285ca"
      },
      "outputs": [],
      "source": [
        "# Before training the model, we save the initial weights so we'll load them\n",
        "# for any training we do to compare the results\n",
        "\n",
        "# now commented so we don't save them again\n",
        "#initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\n",
        "print(initial_weights)\n",
        "#model.save_weights(initial_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUz5gu0Q1_8Z"
      },
      "outputs": [],
      "source": [
        "# TODO: train your model with and without the biais trick and compare the results\n",
        "# (make sure the other weights are the same in your 2 experiments so it's faire to compare the results)\n",
        "# you can train for only 20 epochs at first to make it shorter and compare the two methods\n",
        "\n",
        "# NB: here you'll use a larger batch_size since you want to make sure there is\n",
        "# at least a few positive samples in each batch (try for example batch_size=2048)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmf3NlX4xypb"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "Don't forget to load your weights \n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "Don't forget to load your weights \n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "you can use early stopping \n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "model.load_weights(initial_weights)\n",
        "BATCH_SIZE = 2048\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', \n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    mode='max',\n",
        "    restore_best_weights=True)\n",
        "\n",
        "\n",
        "# ### CASE 1: without the biais\n",
        "\n",
        "model = build_model()\n",
        "model.load_weights(initial_weights)\n",
        "model.layers[-1].bias.assign([0.0])\n",
        "zero_bias_history = model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=20,\n",
        "    validation_data=(val_features, val_labels), \n",
        "    verbose=1)\n",
        "\n",
        "# ### CASE 2: with the biais (it's already part of your saved weights)\n",
        "\n",
        "model = build_model()\n",
        "model.load_weights(initial_weights)\n",
        "careful_bias_history = model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=20,\n",
        "    validation_data=(val_features, val_labels), \n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFgR0vHAAzaM"
      },
      "outputs": [],
      "source": [
        "# TODO: plot the evolution of the errors, we particularly care about the mae and mse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47qz9YEsx0z8"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "\n",
        "zero_bias_history_df = pd.DataFrame(zero_bias_history.history).reset_index().rename(columns={'index': 'epochs'})\n",
        "zero_bias_history_df.tail()\n",
        "\n",
        "careful_bias_history_df = pd.DataFrame(careful_bias_history.history).reset_index().rename(columns={'index': 'epochs'})\n",
        "careful_bias_history_df.tail()\n",
        "\n",
        "metrics_to_plot = ['loss', 'auc', 'precision', 'recall']\n",
        "fig, axes = plt.subplots(len(metrics_to_plot), 1, figsize=(14, 20))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "  ax = axes[i]\n",
        "  # plot zero bias\n",
        "  zero_bias_history_df.plot('epochs', f'{metric}', color='g', label='train zero bias', ax=ax)\n",
        "  zero_bias_history_df.plot('epochs', f'val_{metric}', color='r', label='validation zero bias', ax=ax)\n",
        "  # plot smart bias\n",
        "  careful_bias_history_df.plot('epochs', f'{metric}', color='g', label='train caregful bias', ax=ax, linestyle='--')\n",
        "  careful_bias_history_df.plot('epochs', f'val_{metric}', color='r', label='validation caregful bias', ax=ax, linestyle='--')\n",
        "  ax.set_ylabel(metric)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQXcdVuyBFZO"
      },
      "source": [
        "### Training: baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKf8L0i2CCiW"
      },
      "outputs": [],
      "source": [
        "# TODO: train your model for 100 steps to set a baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVpkmYSpx-w5"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "model = build_model()\n",
        "model.load_weights(initial_weights)\n",
        "baseline_history = model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=100,\n",
        "    callbacks = [early_stopping],\n",
        "    validation_data=(val_features, val_labels))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9sPnj_zCHk1"
      },
      "outputs": [],
      "source": [
        "# TODO: plot the evolution of the errors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRUpaygbx_k4"
      },
      "source": [
        "\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "\n",
        "baseline_history_df = pd.DataFrame(baseline_history.history).reset_index().rename(columns={'index': 'epochs'})\n",
        "baseline_history_df.tail()\n",
        "\n",
        "metrics_to_plot = ['loss', 'auc', 'precision', 'recall']\n",
        "fig, axes = plt.subplots(len(metrics_to_plot), 1, figsize=(14, 20))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "  ax = axes[i]\n",
        "  baseline_history_df.plot('epochs', f'{metric}', color='g', label='train zero bias', ax=ax)\n",
        "  baseline_history_df.plot('epochs', f'val_{metric}', color='r', label='validation zero bias', ax=ax)\n",
        "  ax.set_ylabel(metric)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flopQ9hLCk8E"
      },
      "outputs": [],
      "source": [
        "# TODO: evaluate your model on the test data (compute relevant metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxMptyPYyB-l"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n",
        "test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)\n",
        "\n",
        "baseline_results = model.evaluate(test_features, test_labels,\n",
        "                                  batch_size=BATCH_SIZE, verbose=0)\n",
        "for name, value in zip(model.metrics_names, baseline_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZr3vJPqDAJe"
      },
      "outputs": [],
      "source": [
        "# TODO: plot the confusion matrix, you can use a heatmap as a good way of displaying it\n",
        "# see here https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\n",
        "def plot_cm(labels, predictions, p=0.5):\n",
        "  cm = confusion_matrix(labels, predictions > p)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
        "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
        "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
        "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
        "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFT1V4EsyFgs"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "plot_cm(test_labels, test_predictions_baseline)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9OGdDR8DXQ2"
      },
      "source": [
        "Another relevant curve is the [ROC](https://developers.google.com/machine-learning/glossary#ROC) (Receiver operating characteristic) curve, it's the curve of true positive rate vs. false positive rate at different classification thresholds.\n",
        "\n",
        "The auc metric is the area under the curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hh8qlmJE3Sj"
      },
      "outputs": [],
      "source": [
        "# TODO plot the roc curve\n",
        "# see scikit ref here https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
        "\n",
        "def plot_roc(name, labels, predictions, **kwargs):\n",
        "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
        "\n",
        "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
        "  plt.xlabel('False positives [%]')\n",
        "  plt.ylabel('True positives [%]')\n",
        "  plt.xlim([-0.5,20])\n",
        "  plt.ylim([80,100.5])\n",
        "  plt.grid(True)\n",
        "  ax = plt.gca()\n",
        "  ax.set_aspect('equal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oRMAZH8yPY0"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color='g',)\n",
        "plot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, linestyle='--', color='r')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDqADt82FD3Q"
      },
      "source": [
        "Comment the curve:\n",
        "- Compare precision and recall values.\n",
        "- Think about which error your care the most: for instance here, a false negative (a fraudulent transaction is missed) may have a financial cost, while a false positive (a transaction is incorrectly flagged as fraudulent) may decrease user happiness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCDqefmOGEmD"
      },
      "source": [
        "### Training: class weights\n",
        "\n",
        "If there is a class which you want to emphasize in your training, you can heavily weight it when computing the loss. These will cause the model to \"pay more attention\" to examples from an under-represented class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "zijwEPglGJqO",
        "outputId": "d01bfda0-c50e-4f37-d18f-586af2fa8703"
      },
      "outputs": [],
      "source": [
        "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
        "# The sum of the weights of all examples stays the same.\n",
        "neg = (raw_df.Class == 0).sum()\n",
        "pos = raw_df.Class.sum()\n",
        "total = len(raw_df)\n",
        "\n",
        "weight_for_0 = (1 / neg)*(total)/2.0 \n",
        "weight_for_1 = (1 / pos)*(total)/2.0\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onfPxTbIG3L6"
      },
      "outputs": [],
      "source": [
        "# TODO: train your model with the class weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByvvQ9tJyZWP"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "There is a nice arguments in the keras `.fit()` method called `class_weight`\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "weighted_model = build_model()\n",
        "weighted_model.load_weights(initial_weights)\n",
        "\n",
        "weighted_history = weighted_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=100,\n",
        "    callbacks = [early_stopping],\n",
        "    validation_data=(val_features, val_labels),\n",
        "    # The class weights go here\n",
        "    class_weight=class_weight) \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1eASaczHJBa"
      },
      "outputs": [],
      "source": [
        "# TODO: plot the evolution of the errorsn and evaluate your model on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py1Anixlyaue"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "metrics_to_plot = ['loss', 'auc', 'precision', 'recall']\n",
        "\n",
        "weighted_history_df = pd.DataFrame(weighted_history.history).reset_index().rename(columns={'index': 'epochs'})\n",
        "\n",
        "fig, axes = plt.subplots(len(metrics_to_plot), 1, figsize=(14, 20))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "  ax = axes[i]\n",
        "  baseline_history_df.plot('epochs', f'{metric}', color='g', label='train baseline', ax=ax)\n",
        "  baseline_history_df.plot('epochs', f'val_{metric}', color='r', label='validation baseline', ax=ax)\n",
        "\n",
        "  weighted_history_df.plot('epochs', f'{metric}', color='g', label='train weighted', ax=ax, linestyle='--')\n",
        "  weighted_history_df.plot('epochs', f'val_{metric}', color='r', label='validation weighted', ax=ax, linestyle='--')\n",
        "  ax.set_ylabel(metric)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluation on the test set\n",
        "train_predictions_weighted = weighted_model.predict(train_features, batch_size=BATCH_SIZE)\n",
        "test_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH_SIZE)\n",
        "\n",
        "weighted_results = weighted_model.evaluate(test_features, test_labels,\n",
        "                                           batch_size=BATCH_SIZE, verbose=0)\n",
        "for name, value in zip(weighted_model.metrics_names, weighted_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color='g')\n",
        "plot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color='r')\n",
        "\n",
        "plot_roc(\"Train Weighted\", train_labels, train_predictions_weighted, color='g', linestyle='--')\n",
        "plot_roc(\"Test Weighted\", test_labels, test_predictions_weighted, color='r', linestyle='--')\n",
        "\n",
        "\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJmPEBQ1IvgY"
      },
      "source": [
        "Comment the curves and metrics:\n",
        "- How do the weights impact the different metrics?\n",
        "- Is this new model more relevant for your task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cjbNrlDJC3T"
      },
      "source": [
        "### Training: Oversampling\n",
        "\n",
        "Another popular approach to deal with highly imbalanced data is to modify the ratio of positive samples the model is seeing during the training phase. There are different ways of doing this, we can **oversample** the minority class or **undersample** the majority class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-OwjMZlJZ8m"
      },
      "outputs": [],
      "source": [
        "# First we separate the pos and neg sample\n",
        "\n",
        "pos_features = train_features[bool_train_labels]\n",
        "neg_features = train_features[~bool_train_labels]\n",
        "\n",
        "pos_labels = train_labels[bool_train_labels]\n",
        "neg_labels = train_labels[~bool_train_labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TuQnfYiK8-U"
      },
      "outputs": [],
      "source": [
        "# TODO: use numpy random sampling method with replacement to oversample the positive classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb6O2NS5yftV"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "check the method np.random.choice\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "Oversample to a positive ratio of 30% (if enough time you can try with other rates)\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "Then use np.concatenate to reassemble your dataset and shuffle it\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "Don't forget to update your initial bias and you change the number of samples so it might take longer to run an epoch\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "ids = np.arange(len(pos_features))\n",
        "oversampled_indices = np.random.choice(ids, int(0.3 * len(neg_features)), replace=True)\n",
        "\n",
        "res_pos_features = pos_features[oversampled_indices]\n",
        "res_pos_labels = pos_labels[oversampled_indices]\n",
        "\n",
        "oversampled_features = np.concatenate([res_pos_features, neg_features], axis=0)\n",
        "oversampled_labels = np.concatenate([res_pos_labels, neg_labels], axis=0)\n",
        "\n",
        "order = np.arange(len(oversampled_labels))\n",
        "np.random.shuffle(order)\n",
        "\n",
        "oversampled_features = oversampled_features[order]\n",
        "oversampled_labels = oversampled_labels[order]\n",
        "\n",
        "oversampled_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pQYZ5BrLn9J"
      },
      "outputs": [],
      "source": [
        "# TODO: train your model for 100 epochs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFJvVzRl2s7x"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "oversampled_model = build_model()\n",
        "oversampled_model.load_weights(initial_weights)\n",
        "\n",
        "# Update the init biais\n",
        "pos = bool_train_labels.sum()\n",
        "neg = (~bool_train_labels).sum()\n",
        "\n",
        "oversampled_initial_bias = np.log([oversampled_labels.sum()/(oversampled_labels == 0).sum()])\n",
        "\n",
        "output_layer = oversampled_model.layers[-1] \n",
        "output_layer.bias.assign(oversampled_initial_bias)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).cache()\n",
        "val_ds = val_ds.batch(BATCH_SIZE).prefetch(2) \n",
        "\n",
        "oversampled_history = oversampled_model.fit(\n",
        "    oversampled_features,\n",
        "    oversampled_labels,\n",
        "    epochs=100,\n",
        "    callbacks = [early_stopping],\n",
        "    validation_data=(val_features, val_labels))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcrZTkq1ODoM"
      },
      "outputs": [],
      "source": [
        "# TODO: plot the evolution of the errors and compare it with previous methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-O5ymx127H7"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "metrics_to_plot = ['loss', 'auc', 'precision', 'recall']\n",
        "\n",
        "oversampled_history_df = pd.DataFrame(oversampled_history.history).reset_index().rename(columns={'index': 'epochs'})\n",
        "\n",
        "fig, axes = plt.subplots(len(metrics_to_plot), 1, figsize=(14, 20))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "  ax = axes[i]\n",
        "  baseline_history_df.plot('epochs', f'{metric}', color='g', label='train baseline', ax=ax)\n",
        "  baseline_history_df.plot('epochs', f'val_{metric}', color='r', label='validation baseline', ax=ax)\n",
        "\n",
        "  weighted_history_df.plot('epochs', f'{metric}', color='g', label='train weighted', ax=ax, linestyle='--')\n",
        "  weighted_history_df.plot('epochs', f'val_{metric}', color='r', label='validation weighted', ax=ax, linestyle='--')\n",
        "\n",
        "  oversampled_history_df.plot('epochs', f'{metric}', color='g', label='train weighted', ax=ax, linestyle='dotted')\n",
        "  oversampled_history_df.plot('epochs', f'val_{metric}', color='r', label='validation weighted', ax=ax, linestyle='dotted')\n",
        "  ax.set_ylabel(metric)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on test data and plot the ROC curve and compare it with previous methods\n",
        "\n",
        "train_predictions_oversampled = oversampled_model.predict(train_features, batch_size=BATCH_SIZE)\n",
        "test_predictions_oversampled = oversampled_model.predict(test_features, batch_size=BATCH_SIZE)\n",
        "\n",
        "oversampled_results = oversampled_model.evaluate(test_features, test_labels,\n",
        "                                             batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "for name, value in zip(oversampled_model.metrics_names, oversampled_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "\n",
        "plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color='g')\n",
        "plot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color='r')\n",
        "\n",
        "plot_roc(\"Train Weighted\", train_labels, train_predictions_weighted, color='g', linestyle='--')\n",
        "plot_roc(\"Test Weighted\", test_labels, test_predictions_weighted, color='r', linestyle='--')\n",
        "\n",
        "plot_roc(\"Train Oversampled\", train_labels, train_predictions_oversampled,  color='g', linestyle='dotted')\n",
        "plot_roc(\"Test Oversampled\", test_labels, test_predictions_oversampled,  color='r', linestyle='dotted')\n",
        "plt.legend(loc='lower right')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl_ywEHEPvIh"
      },
      "source": [
        "Comment your results. You can also update the oversampling ratio and see how it affects your test metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKBz7DNlPm33"
      },
      "source": [
        "### Training: Undersampling\n",
        "\n",
        "Check out the effect of the undersampling strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBbpRibdPueA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "day4_homework_logistic.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('dforb')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "0e9ae77a68d479a28967a79b3dd1a040f581e32c13de9b5e9a3a6cf002e7768e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
